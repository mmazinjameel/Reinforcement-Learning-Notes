{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8ff306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Environment parameters\n",
    "gamma = 0.9\n",
    "noise = 0.2\n",
    "living_reward = 0\n",
    "\n",
    "# Grid dimensions\n",
    "rows, cols = 3, 4\n",
    "\n",
    "# Define rewards grid\n",
    "rewards = np.zeros((rows, cols))\n",
    "rewards[0, 3] = +1\n",
    "rewards[1, 3] = -1\n",
    "rewards[1, 1] = None  # Wall\n",
    "\n",
    "# Terminal and wall states\n",
    "terminal_states = [(0, 3), (1, 3)]\n",
    "wall_states = [(1, 1)]\n",
    "\n",
    "# Actions and their vectors\n",
    "actions = ['U', 'D', 'L', 'R']\n",
    "action_vectors = {\n",
    "    'U': (-1, 0),\n",
    "    'D': (1, 0),\n",
    "    'L': (0, -1),\n",
    "    'R': (0, 1)\n",
    "}\n",
    "\n",
    "# Utility function to check bounds\n",
    "def in_bounds(state):\n",
    "    r, c = state\n",
    "    return 0 <= r < rows and 0 <= c < cols and (r, c) not in wall_states\n",
    "\n",
    "# Compute expected value of taking an action at a state\n",
    "def compute_action_value(state, action, V):\n",
    "    r, c = state\n",
    "    primary_move = action_vectors[action]\n",
    "\n",
    "    if action in ['U', 'D']:\n",
    "        sideways = ['L', 'R']\n",
    "    else:\n",
    "        sideways = ['U', 'D']\n",
    "\n",
    "    moves = [(primary_move, 1 - noise)] + [(action_vectors[a], noise / 2) for a in sideways]\n",
    "\n",
    "    value = 0\n",
    "    for move, prob in moves:\n",
    "        new_r, new_c = r + move[0], c + move[1]\n",
    "        if not in_bounds((new_r, new_c)):\n",
    "            new_r, new_c = r, c\n",
    "        reward = living_reward if (new_r, new_c) not in terminal_states else rewards[new_r, new_c]\n",
    "        value += prob * (reward + gamma * V[new_r, new_c])\n",
    "    return value\n",
    "\n",
    "# Initialize policy randomly\n",
    "policy = np.full((rows, cols), 'U', dtype=object)\n",
    "for r, c in terminal_states + wall_states:\n",
    "    policy[r, c] = None\n",
    "\n",
    "# Initialize value function\n",
    "V = np.zeros((rows, cols))\n",
    "\n",
    "# Policy Iteration\n",
    "is_policy_stable = False\n",
    "iteration = 0\n",
    "\n",
    "while not is_policy_stable:\n",
    "    # Policy Evaluation\n",
    "    while True:\n",
    "        delta = 0\n",
    "        new_V = np.copy(V)\n",
    "        for r in range(rows):\n",
    "            for c in range(cols):\n",
    "                if (r, c) in terminal_states or (r, c) in wall_states:\n",
    "                    continue\n",
    "                v = compute_action_value((r, c), policy[r, c], V)\n",
    "                new_V[r, c] = v\n",
    "                delta = max(delta, abs(v - V[r, c]))\n",
    "        V = new_V\n",
    "        if delta < 1e-4:\n",
    "            break\n",
    "\n",
    "    # Policy Improvement\n",
    "    is_policy_stable = True\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            if (r, c) in terminal_states or (r, c) in wall_states:\n",
    "                continue\n",
    "            old_action = policy[r, c]\n",
    "            action_values = [compute_action_value((r, c), a, V) for a in actions]\n",
    "            best_action = actions[np.argmax(action_values)]\n",
    "            policy[r, c] = best_action\n",
    "            if best_action != old_action:\n",
    "                is_policy_stable = False\n",
    "\n",
    "import pandas as pd\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Policy Iteration - Value Function\", dataframe=pd.DataFrame(V))\n",
    "\n",
    "policy_display = pd.DataFrame(policy)\n",
    "tools.display_dataframe_to_user(name=\"Policy Iteration - Policy\", dataframe=policy_display)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
